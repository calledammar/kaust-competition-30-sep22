{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ammar1almutairi/2022-11-30-assessment?scriptVersionId=114949376\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import pathlib\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import compose, impute, linear_model, model_selection, pipeline, preprocessing \nimport torch\nfrom torch import nn, optim, utils\nimport torchmetrics\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-01T08:27:50.439689Z","iopub.execute_input":"2022-12-01T08:27:50.440261Z","iopub.status.idle":"2022-12-01T08:27:50.447127Z","shell.execute_reply.started":"2022-12-01T08:27:50.440221Z","shell.execute_reply":"2022-12-01T08:27:50.446239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = pathlib.Path(\"/kaggle/input/kaust-academy-ai-week-november-2022\")\nWORKING_DIR = pathlib.Path(\"/kaggle/working\")","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:27:51.159739Z","iopub.execute_input":"2022-12-01T08:27:51.161337Z","iopub.status.idle":"2022-12-01T08:27:51.166662Z","shell.execute_reply.started":"2022-12-01T08:27:51.161277Z","shell.execute_reply":"2022-12-01T08:27:51.165487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load the training data","metadata":{}},{"cell_type":"code","source":"_train_df = pd.read_csv(f\"{INPUT_DIR}/train.csv\")\n\n# need to have some validation data\n_seed = 42\ntrain_df, val_df = model_selection.train_test_split(\n    _train_df,\n    test_size=0.1,\n    random_state = np.random.RandomState(_seed),\n    stratify=_train_df.loc[:, \"Transported\"],\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:27:51.990609Z","iopub.execute_input":"2022-12-01T08:27:51.991088Z","iopub.status.idle":"2022-12-01T08:27:52.037291Z","shell.execute_reply.started":"2022-12-01T08:27:51.991051Z","shell.execute_reply":"2022-12-01T08:27:52.036041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Divide the training (validation) features from the training (validation) target ","metadata":{}},{"cell_type":"code","source":"train_features = train_df.drop(\"Transported\", axis=1)\ntrain_target = train_df[\"Transported\"]\n\nval_features = val_df.drop(\"Transported\", axis=1)\nval_target = val_df[\"Transported\"]","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:27:52.600071Z","iopub.execute_input":"2022-12-01T08:27:52.600468Z","iopub.status.idle":"2022-12-01T08:27:52.611603Z","shell.execute_reply.started":"2022-12-01T08:27:52.600435Z","shell.execute_reply":"2022-12-01T08:27:52.610242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data preprocessing","metadata":{}},{"cell_type":"code","source":"boolean_preprocessing = pipeline.make_pipeline(\n    impute.SimpleImputer(strategy=\"most_frequent\"),\n)\n\ncategorical_preprocessing = pipeline.make_pipeline(\n    impute.SimpleImputer(strategy=\"most_frequent\"),\n    preprocessing.OneHotEncoder(),\n)\n\nnumeric_preprocessing = pipeline.make_pipeline(\n    impute.SimpleImputer(strategy=\"mean\")\n)\n\nto_torch_tensor = pipeline.make_pipeline(\n    preprocessing.FunctionTransformer(lambda arr: arr.astype(np.float32)),\n    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n)\n\nfeature_column_transformer = compose.make_column_transformer(\n    (boolean_preprocessing, [\"CryoSleep\", \"VIP\"]),\n    (categorical_preprocessing, [\"HomePlanet\", \"Destination\"]),\n    (numeric_preprocessing, compose.make_column_selector(dtype_include=np.float64)),\n    remainder = \"drop\",\n)\n\nfeature_preprocessing = pipeline.make_pipeline(\n    feature_column_transformer ,\n    to_torch_tensor\n)\n\ntarget_preprocessing = pipeline.make_pipeline(\n    preprocessing.FunctionTransformer(lambda df: df.to_numpy()),\n    to_torch_tensor\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:27:53.094444Z","iopub.execute_input":"2022-12-01T08:27:53.094963Z","iopub.status.idle":"2022-12-01T08:27:53.107977Z","shell.execute_reply.started":"2022-12-01T08:27:53.094921Z","shell.execute_reply":"2022-12-01T08:27:53.106827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Create your datasets and dataloaders","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_WORKERS = 4\n\ntrain_features_tensor = feature_preprocessing.fit_transform(train_features)\ntrain_target_tensor = target_preprocessing.fit_transform(train_target)\n\ntrain_dataset = utils.data.TensorDataset(train_features_tensor, train_target_tensor)\ntrain_dataloader = utils.data.DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n)\n\nval_features_tensor = feature_preprocessing.transform(val_features)\nval_target_tensor = target_preprocessing.transform(val_target)\n\nval_dataset = utils.data.TensorDataset(val_features_tensor, val_target_tensor)\nval_dataloader = utils.data.DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:27:54.090515Z","iopub.execute_input":"2022-12-01T08:27:54.091181Z","iopub.status.idle":"2022-12-01T08:27:54.150541Z","shell.execute_reply.started":"2022-12-01T08:27:54.091142Z","shell.execute_reply":"2022-12-01T08:27:54.149232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Define a multi-layer perceptron classifier","metadata":{}},{"cell_type":"code","source":"_, in_features = train_features_tensor.shape\nhidden_features = [14,256,128,64,32,1]\n\nmodel_fn = nn.Sequential(\n    nn.Linear(14,128),\n    nn.Dropout(0.1),\n    nn.Linear(128,64),\n    nn.LeakyReLU(0.1),\n    nn.Linear(64,32),\n    nn.LeakyReLU(0.1),\n    nn.Linear(32,16),\n    nn.Dropout(0.1),\n    nn.LeakyReLU(0.1),\n    nn.Linear(16,10),\n    nn.LeakyReLU(0.1),\n    nn.Linear(10,8),\n    nn.LeakyReLU(0.1),\n    nn.Linear(8,4),\n    nn.LeakyReLU(0.1),\n    nn.Linear(4,1),\n    nn.Sigmoid()\n)\n\nprint(model_fn)\n\nloss_fn = nn.BCELoss()\n\n_optimizer_kwargs = {\n    \"momentum\": 0.9,\n    \"nesterov\": False,\n}\noptimizer = optim.SGD(model_fn.parameters(), lr=0.001, **_optimizer_kwargs)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:29:40.729953Z","iopub.execute_input":"2022-12-01T08:29:40.73049Z","iopub.status.idle":"2022-12-01T08:29:40.746601Z","shell.execute_reply.started":"2022-12-01T08:29:40.730439Z","shell.execute_reply":"2022-12-01T08:29:40.744744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Train your classifier","metadata":{}},{"cell_type":"code","source":"epochs = 141\nlog_epochs = 20\n\n\nmodel_fn.train()\nfor epoch in range(epochs):\n    \n    train_losses = []\n    for features, targets in train_dataloader:\n        \n        # forward pass\n        predictions = model_fn(features) \n        predictions = predictions.squeeze(1)\n        train_loss = loss_fn(predictions, targets)\n        train_losses.append(train_loss)\n        \n        # backward pass\n        train_loss.backward()        \n        optimizer.step()        \n        optimizer.zero_grad()\n        \n    train_loss = (torch.stack(train_losses)\n                       .mean())\n    \n    \n    \n    with torch.no_grad():\n        \n        model_fn.eval()\n        \n        val_losses = []\n        for features, targets in val_dataloader:\n            predictions = model_fn(features)\n            predictions = predictions.squeeze(1)\n            val_loss = loss_fn(predictions, targets)\n            val_losses.append(val_loss)\n    \n        val_loss = (torch.stack(val_losses)\n                         .mean())\n\n    if epoch % log_epochs == 0:\n        print(f'Epoch {epoch}, Training Loss {train_loss.item():.4f}, Validation Loss {val_loss.item():.4f}')\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:30:08.331005Z","iopub.execute_input":"2022-12-01T08:30:08.331454Z","iopub.status.idle":"2022-12-01T08:35:08.06142Z","shell.execute_reply.started":"2022-12-01T08:30:08.33142Z","shell.execute_reply":"2022-12-01T08:35:08.059461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Load the testing features","metadata":{}},{"cell_type":"code","source":"test_features = pd.read_csv(f\"{INPUT_DIR}/test.csv\")\ntest_data = feature_preprocessing.transform(test_features)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:11:19.901595Z","iopub.status.idle":"2022-12-01T08:11:19.902054Z","shell.execute_reply.started":"2022-12-01T08:11:19.901838Z","shell.execute_reply":"2022-12-01T08:11:19.901858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Make predictions using the test features","metadata":{}},{"cell_type":"code","source":"features_tensor = feature_preprocessing.transform(test_features)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:11:19.904088Z","iopub.status.idle":"2022-12-01T08:11:19.904982Z","shell.execute_reply.started":"2022-12-01T08:11:19.904692Z","shell.execute_reply":"2022-12-01T08:11:19.904721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    probas = model_fn(features_tensor)\npredictions = probas[:, 0] > 0.5","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:11:19.906672Z","iopub.status.idle":"2022-12-01T08:11:19.907149Z","shell.execute_reply.started":"2022-12-01T08:11:19.906941Z","shell.execute_reply":"2022-12-01T08:11:19.906964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Load the sample submission file ","metadata":{}},{"cell_type":"code","source":"test_features = test_features.set_index('PassengerId')\nsample_submission_df = test_features","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:11:19.909096Z","iopub.status.idle":"2022-12-01T08:11:19.909697Z","shell.execute_reply.started":"2022-12-01T08:11:19.909498Z","shell.execute_reply":"2022-12-01T08:11:19.909519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Create the submission file","metadata":{}},{"cell_type":"code","source":"_ = (pd.DataFrame({\"Transported\": predictions}, index=sample_submission_df.index)\n       .to_csv(WORKING_DIR / \"submission.csv\"))  ","metadata":{"execution":{"iopub.status.busy":"2022-12-01T08:11:19.910662Z","iopub.status.idle":"2022-12-01T08:11:19.911528Z","shell.execute_reply.started":"2022-12-01T08:11:19.911278Z","shell.execute_reply":"2022-12-01T08:11:19.9113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}